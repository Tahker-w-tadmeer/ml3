{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:24:40.626071Z",
     "start_time": "2023-12-21T09:24:39.990022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.56516688 0.92705525 0.28571429 ... 0.61111111 0.56427143 1.        ]\n",
      " [0.63358586 0.49312747 0.35714286 ... 0.73333333 0.65321251 0.        ]\n",
      " [0.71162294 0.81947715 0.5        ... 0.42222222 0.65321251 1.        ]\n",
      " ...\n",
      " [0.38907563 0.44179874 0.14285714 ... 0.37777778 0.39794001 0.        ]\n",
      " [0.63358586 0.74632457 0.5        ... 0.45555556 0.56427143 1.        ]\n",
      " [0.46470946 0.61042446 0.07142857 ... 0.5        0.54406804 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tahkeer_data_cleaned.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth  \n",
    "        self.models = [DecisionTreeClassifier(max_depth=self.max_depth) for _ in range(n_estimators)]  \n",
    "    def fit(self, X, y):\n",
    "        for model in self.models:\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            X_subset, y_subset = X.iloc[indices], y.iloc[indices] \n",
    "            model.fit(X_subset, y_subset)\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        predictions = np.zeros((len(X), self.n_estimators))\n",
    "        for i, model in enumerate(self.models):\n",
    "            predictions[:, i] = model.predict(X)\n",
    "        avg_predictions = np.mean(predictions, axis=1)\n",
    "        binary_predictions = (avg_predictions >= threshold).astype(int)\n",
    "        return binary_predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:24:41.581128Z",
     "start_time": "2023-12-21T09:24:40.630401Z"
    }
   },
   "id": "9fac969431223e6e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['triglyceride', 'weight(kg)', 'serum creatinine', 'waist(cm)', 'LDL', 'Cholesterol', 'HDL', 'fasting blood sugar', 'systolic', 'AST', 'smoking']\n",
      "['triglyceride', 'weight(kg)', 'serum creatinine', 'waist(cm)', 'LDL', 'Cholesterol', 'HDL', 'fasting blood sugar', 'systolic', 'AST']\n"
     ]
    }
   ],
   "source": [
    "columns = df.columns.tolist()[1:]\n",
    "print(columns)\n",
    "columns.remove(\"smoking\")\n",
    "print(columns)\n",
    "x=df[columns]\n",
    "y=df[\"smoking\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:24:41.593346Z",
     "start_time": "2023-12-21T09:24:41.582162Z"
    }
   },
   "id": "fe9ab1e20fe86d44"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.30,shuffle=False,train_size=0.70)\n",
    "bagging_model = BaggingClassifier(n_estimators=100, max_depth=60)\n",
    "bagging_model.fit(xtrain, ytrain)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:26:15.992256Z",
     "start_time": "2023-12-21T09:24:41.597610Z"
    }
   },
   "id": "cd37abfe3e698ec6"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Accuracy: 0.712570107462431\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "bagging_predictions = bagging_model.predict(xtest)\n",
    "bagging_accuracy = accuracy_score(ytest, bagging_predictions)\n",
    "print(f\"Bagging Accuracy: {bagging_accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:26:17.356250Z",
     "start_time": "2023-12-21T09:26:15.992110Z"
    }
   },
   "id": "789d1cdb28b22062"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        weights = np.ones(n_samples) / n_samples\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = DecisionStump()\n",
    "            error = 0.0\n",
    "            for feature in range(n_features):\n",
    "                threshold_values = np.unique(X[:, feature])\n",
    "                for threshold in threshold_values:\n",
    "                    pred = model.predict(X, feature, threshold)\n",
    "                    incorrect = pred != y\n",
    "                    weighted_error = np.sum(weights[incorrect])\n",
    "                    if weighted_error < error or error == 0.0:\n",
    "                        error = weighted_error\n",
    "                        model.feature_index = feature\n",
    "                        model.threshold = threshold\n",
    "                        model.prediction = pred.copy()\n",
    "\n",
    "            alpha = 0.5 * np.log((1.0 - error) / max(error, 1e-10))\n",
    "            weights *= np.exp(-alpha * y * model.predict(X))\n",
    "\n",
    "            normalization_factor = np.sum(weights)\n",
    "            weights /= normalization_factor\n",
    "\n",
    "            self.alphas.append(alpha)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = np.array([model.predict(X) for model in self.models])\n",
    "        return np.sign(np.dot(self.alphas, predictions))\n",
    "\n",
    "\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.prediction = None\n",
    "\n",
    "    def predict(self, X, feature=None, threshold=None):\n",
    "        if feature is not None and threshold is not None:\n",
    "            return np.where(X[:, feature] < threshold, -1, 1)\n",
    "        else:\n",
    "            return self.prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T09:27:17.201951Z"
    }
   },
   "id": "54300543aa3d236d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1. -1. -1. ... -1. -1. -1.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [46714, 108999]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(predictions)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Evaluate accuracy\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m accuracy \u001B[38;5;241m=\u001B[39m \u001B[43maccuracy_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43mytest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAccuracy: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/sklearn/utils/_param_validation.py:214\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    208\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    209\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    210\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    211\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    212\u001B[0m         )\n\u001B[1;32m    213\u001B[0m     ):\n\u001B[0;32m--> 214\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    215\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    223\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    224\u001B[0m     )\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/sklearn/metrics/_classification.py:220\u001B[0m, in \u001B[0;36maccuracy_score\u001B[0;34m(y_true, y_pred, normalize, sample_weight)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Accuracy classification score.\u001B[39;00m\n\u001B[1;32m    155\u001B[0m \n\u001B[1;32m    156\u001B[0m \u001B[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    216\u001B[0m \u001B[38;5;124;03m0.5\u001B[39;00m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    219\u001B[0m \u001B[38;5;66;03m# Compute accuracy for each possible representation\u001B[39;00m\n\u001B[0;32m--> 220\u001B[0m y_type, y_true, y_pred \u001B[38;5;241m=\u001B[39m \u001B[43m_check_targets\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    221\u001B[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001B[1;32m    222\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_type\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmultilabel\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/sklearn/metrics/_classification.py:84\u001B[0m, in \u001B[0;36m_check_targets\u001B[0;34m(y_true, y_pred)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_check_targets\u001B[39m(y_true, y_pred):\n\u001B[1;32m     58\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \n\u001B[1;32m     60\u001B[0m \u001B[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;124;03m    y_pred : array or indicator matrix\u001B[39;00m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 84\u001B[0m     \u001B[43mcheck_consistent_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     85\u001B[0m     type_true \u001B[38;5;241m=\u001B[39m type_of_target(y_true, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     86\u001B[0m     type_pred \u001B[38;5;241m=\u001B[39m type_of_target(y_pred, input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124my_pred\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/Library/Python/3.9/site-packages/sklearn/utils/validation.py:407\u001B[0m, in \u001B[0;36mcheck_consistent_length\u001B[0;34m(*arrays)\u001B[0m\n\u001B[1;32m    405\u001B[0m uniques \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(lengths)\n\u001B[1;32m    406\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(uniques) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 407\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    408\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound input variables with inconsistent numbers of samples: \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    409\u001B[0m         \u001B[38;5;241m%\u001B[39m [\u001B[38;5;28mint\u001B[39m(l) \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m lengths]\n\u001B[1;32m    410\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: Found input variables with inconsistent numbers of samples: [46714, 108999]"
     ]
    }
   ],
   "source": [
    "adaboost = AdaBoost(n_estimators=50)\n",
    "adaboost.fit(xtrain, ytrain)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = adaboost.predict(xtest)\n",
    "\n",
    "print(predictions)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T09:26:17.368735Z"
    }
   },
   "id": "d0b842f92c466fda"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        n_samples, n_features = X.shape\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Randomly select a subset of features\n",
    "            selected_features = np.random.choice(n_features, size=int(np.sqrt(n_features)), replace=False)\n",
    "            X_subset = X[:, selected_features]\n",
    "\n",
    "            # Create a decision tree with random features\n",
    "            tree = DecisionTreeClassifier(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf\n",
    "            )\n",
    "            tree.fit(X_subset, y)\n",
    "            self.models.append((tree, selected_features))\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.array(X)\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators))\n",
    "        for i, (tree, selected_features) in enumerate(self.models):\n",
    "            X_subset = X[:, selected_features]\n",
    "            predictions[:, i] = tree.predict(X_subset)\n",
    "\n",
    "        # Use majority voting for the final prediction\n",
    "        final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "        return final_predictions\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:27:17.195224Z",
     "start_time": "2023-12-21T09:27:17.187328Z"
    }
   },
   "id": "f4260778e822584a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "random_forest = RandomForestClassifier(n_estimators=50, max_depth=None, min_samples_split=2, min_samples_leaf=1)\n",
    "random_forest.fit(xtrain, ytrain)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = random_forest.predict(xtest)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(ytest, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T09:27:17.194438Z"
    }
   },
   "id": "571c8202d45bc0c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-21T09:27:17.198968Z"
    }
   },
   "id": "ece19304eaa1bb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
